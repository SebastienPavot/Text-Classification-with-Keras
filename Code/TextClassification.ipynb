{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600448304569",
   "display_name": "Python 3.7.7 64-bit ('tensorFlow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Corona Tweets Classification using Keras:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Package & Libraries:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "source": [
    "## Data reading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/spavot/Documents/Perso/Text classification & Visualization/Data/Corona_NLP_train.csv', encoding = 'latin')\n",
    "test = pd.read_csv('/Users/spavot/Documents/Perso/Text classification & Visualization/Data/Corona_NLP_test.csv', encoding ='latin')"
   ]
  },
  {
   "source": [
    "## Data exploration:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "source": [
    "* We see that we have 41157 values but we have only 32k non null values for location, we will have to fix this:\n",
    "\n",
    "* UserName and ScreenName are id related data, we won't use it\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's see which location is the more popular:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = train.Location\n",
    "location = pd.DataFrame(location)\n",
    "location['Count'] = 1\n",
    "location = location.groupby('Location').sum().sort_values(by = 'Count', ascending = False).nlargest(15,['Count'])\n",
    "location = location.reset_index()\n",
    "plt.figure(figsize=(25,7))\n",
    "sns.barplot(x = 'Count', y = 'Location', data = location)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We can see that we have some noises and some location are country where other are cities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we want to have a look of the distribution of our target variable:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,7))\n",
    "sns.countplot(train.Sentiment)"
   ]
  },
  {
   "source": [
    "Seems like the target variable distribution is not skewed and we doesn't risk to have a category which is never predicted due to the lack of presence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's look at the distribution of tweets over time:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = train.TweetAt\n",
    "time = pd.DataFrame(time)\n",
    "time['Count'] = 1\n",
    "time = time.groupby('TweetAt').sum()\n",
    "time = time.reset_index()\n",
    "time = time.iloc[1:,:]\n",
    "time['TweetAt'] = pd.to_datetime(time['TweetAt'], format = '%d-%m-%Y')\n",
    "plt.figure(figsize=(25,7))\n",
    "sns.lineplot(x = 'TweetAt', y = 'Count', data = time)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Min = time.TweetAt.min()\n",
    "Max = time.TweetAt.max()\n",
    "print(f'The date range of the data is between {Min} and {Max}')"
   ]
  },
  {
   "source": [
    "Seems like we have some day without data and some with a lot of tweets. The tweets are spread between 16 of March to 14 of April"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Finally, let's analyze the lengths of tweets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length_tweets = pd.DataFrame(train.OriginalTweet)\n",
    "length_measured = []\n",
    "for i in length_tweets.OriginalTweet:\n",
    " length_measured.append(len(i))\n",
    "\n",
    "plt.figure(figsize=(25,7))\n",
    "sns.distplot(length_measured)"
   ]
  },
  {
   "source": [
    "It seems that the repartition is pretty well distributed, we can see a raise at 260 characters but overall we have tweets of all size. Note that we will have to check the length after processing the tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data cleaning:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Drop variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's first drop UserName and ScreenName as they are only id variables so we won't use them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['UserName','ScreenName'], axis = 1)\n",
    "test = test.drop(['UserName','ScreenName'], axis = 1)"
   ]
  },
  {
   "source": [
    "### Location variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we will replace \"NA\" values with \"Unknown\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Location = train.Location.fillna('Unknown')"
   ]
  },
  {
   "source": [
    "Now we import a dataframe containg major cities in the word and countries in order join on it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_country = pd.read_csv('/Users/spavot/Documents/Perso/Text classification & Visualization/Data/world-cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = []\n",
    "\n",
    "# for i in train.Location:\n",
    "#     for y in city_country.name:\n",
    "#         if y.lower() in i.lower():\n",
    "#             city_name = y\n",
    "#         else:\n",
    "#             city_name = 'Unknown'\n",
    "#     city_list.append(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_list = pd.DataFrame(city_list)\n",
    "# city_list.value_counts()"
   ]
  },
  {
   "source": [
    "We transform the TweetAt column into time variable:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TweetAt Variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TweetAt'] = pd.to_datetime(train['TweetAt'], format = '%d-%m-%Y')\n",
    "test['TweetAt'] = pd.to_datetime(test['TweetAt'], format = '%d-%m-%Y')"
   ]
  },
  {
   "source": [
    "### Original tweet cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we will start fixing the tweets, we need to remove the punctuation and specific characters etc.. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.OriginalTweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform to lower:\n",
    "train.OriginalTweet = train.OriginalTweet.str.lower()\n",
    "test.OriginalTweet = test.OriginalTweet.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove urls:\n",
    "train.OriginalTweet = train.OriginalTweet.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "test.OriginalTweet = test.OriginalTweet.str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extract hastags & append them into a new column:\n",
    "for data in train, test:\n",
    "    Hashtags = []\n",
    "    for i in range(0,len(data.OriginalTweet)):\n",
    "        if len(re.findall(r\"#(\\w+)\", data.OriginalTweet[i]))>0:\n",
    "            Hashtags.append(re.findall(r\"#(\\w+)\", data.OriginalTweet[i]))\n",
    "        else:\n",
    "            Hashtags.append('None')\n",
    "    data['Hashtags'] =  Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace hashtags text now that we extracted it\n",
    "train.OriginalTweet = train.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word[0] != '#']))\n",
    "test.OriginalTweet = test.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word[0] != '#']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation, special characters & mentions:\n",
    "train.OriginalTweet = train.OriginalTweet.str.replace(r'[^\\w\\s]', '', case=False)\n",
    "test.OriginalTweet = test.OriginalTweet.str.replace(r'[^\\w\\s]', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove stopwords:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train.OriginalTweet = train.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "test.OriginalTweet = test.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non alphabetic words:\n",
    "train.OriginalTweet = train.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test.OriginalTweet = test.OriginalTweet.apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove emptys rows:\n",
    "train = train[train.OriginalTweet != '']\n",
    "test = test[test.OriginalTweet != '']"
   ]
  },
  {
   "source": [
    "Let's check if the cleaning seems okay:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(i,':',train.OriginalTweet[i])\n",
    "    print(i,':',test.OriginalTweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "source": [
    "It seems ðŸ‘Œ, now let's go into the data preparation for our model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Preparation for the model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the text into number using Count Vectorizer from sickitlearn:\n",
    "train_text = train.OriginalTweet.values\n",
    "test_text = test.OriginalTweet.values\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training / test set:\n",
    "X_train = vectorizer.transform(train_text)\n",
    "X_test  = vectorizer.transform(test_text)\n",
    "y_train = pd.get_dummies(train.Sentiment).values\n",
    "y_test = pd.get_dummies(test.Sentiment).values\n"
   ]
  },
  {
   "source": [
    "## Modeling Deep Neural Network with Keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simple one layer Model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(30, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model_simple.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_simple.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_simple = model_simple.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=25)"
   ]
  },
  {
   "source": [
    "We now initiate a function to plot the learning evolution of our first model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, model):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x, acc, label='Training acc')\n",
    "    sns.lineplot(x, val_acc, label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x, loss, label='Training loss')\n",
    "    sns.lineplot(x, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history_simple, model_simple)"
   ]
  },
  {
   "source": [
    "Seems like our model overfit really fast, we end with a test score of 0.62 where we have 0.98 with training data, let's try different types of model and see if we can increase performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Multi layers model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = Sequential()\n",
    "multi_model.add(Dense(64, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "multi_model.add(Dense(32, activation = 'relu'))\n",
    "multi_model.add(Dense(16, activation= 'relu'))\n",
    "multi_model.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "multi_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_Multi = multi_model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history_Multi, multi_model)"
   ]
  },
  {
   "source": [
    "We improved the score with more layers, it looks like with more layers the model can learn more from the data but we still suffer from an overfitting really fast, let's try to change the way we encoded the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Words embeddings to improve the model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Embedding, Flatten, GlobalMaxPool1D, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "X_train = tokenizer.texts_to_sequences(train_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print(train_text[1])\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "multi_model_Embed = Sequential()\n",
    "multi_model_Embed.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "multi_model_Embed.add(Flatten())\n",
    "multi_model_Embed.add(Dense(64, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(32, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(16, activation = 'relu'))\n",
    "multi_model_Embed.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_model_Embed.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "multi_model_Embed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_Multi_Embed = multi_model_Embed.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history_Multi_Embed, multi_model_Embed)"
   ]
  },
  {
   "source": [
    "Add GlobalMaxPool:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_Embed_Max = Sequential()\n",
    "multi_model_Embed_Max.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "multi_model_Embed_Max.add(GlobalMaxPool1D())\n",
    "multi_model_Embed_Max.add(Dense(64, activation = 'relu'))\n",
    "multi_model_Embed_Max.add(Dense(32, activation = 'relu'))\n",
    "multi_model_Embed_Max.add(Dense(16, activation = 'relu'))\n",
    "multi_model_Embed_Max.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_model_Embed_Max.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "multi_model_Embed_Max.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Multi_Embed_Max = multi_model_Embed_Max.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(Multi_Embed_Max, multi_model_Embed_Max)"
   ]
  },
  {
   "source": [
    "Convolutional Neural Networks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Conv = Sequential()\n",
    "model_Conv.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model_Conv.add(Conv1D(128, 5, activation='relu'))\n",
    "model_Conv.add(GlobalMaxPool1D())\n",
    "model_Conv.add(Dense(64, activation = 'relu'))\n",
    "model_Conv.add(Dense(32, activation = 'relu'))\n",
    "model_Conv.add(Dense(16, activation = 'relu'))\n",
    "model_Conv.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_Conv.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model_Conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_Conv = model_Conv.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history_Conv, model_Conv)"
   ]
  },
  {
   "source": [
    "Hyperparameters tuning:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model_Conv = Sequential()\n",
    "    model_Conv.add(Embedding(input_dim=vocab_size, \n",
    "                            output_dim=embedding_dim, \n",
    "                            input_length=maxlen))\n",
    "    model_Conv.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model_Conv.add(GlobalMaxPool1D())\n",
    "    model_Conv.add(Dense(64, activation = 'relu'))\n",
    "    model_Conv.add(Dense(32, activation = 'relu'))\n",
    "    model_Conv.add(Dense(16, activation = 'relu'))\n",
    "    model_Conv.add(Dense(5, activation = 'softmax'))\n",
    "    model_Conv.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    return model_Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  vocab_size=[45929], \n",
    "                  embedding_dim=[50],\n",
    "                  maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model_grid = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, batch_size=10,\n",
    "                            verbose=True)\n",
    "grid = RandomizedSearchCV(estimator=model_grid, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate testing set\n",
    "test_accuracy = grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}